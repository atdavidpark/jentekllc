{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "          .builder \\\n",
    "          .config(\"spark.master\", \"local\") \\\n",
    "          .appName(\"interfacing spark sql to hive metastore with no configuration file\") \\\n",
    "          .config(\"hive.metastore.uris\", \"thrift://10.0.0.46:9083\") \\\n",
    "          .enableHiveSupport() \\\n",
    "          .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"LOAD DATA LOCAL INPATH 'D:/spark/examples/src/main/resources/kv1.txt' INTO TABLE src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|key|value  |\n",
      "+---+-------+\n",
      "|238|val_238|\n",
      "|86 |val_86 |\n",
      "|311|val_311|\n",
      "+---+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM src\").show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     500|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT COUNT(*) FROM src\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlDF = spark.sql(\"SELECT key, value FROM src WHERE key < 10 ORDER BY key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringsDS = sqlDF.rdd.map(lambda row: \"Key: %d, Value: %s\" % (row.key, row.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(stringsDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: val_0\n",
      "Key: 2, Value: val_2\n",
      "Key: 4, Value: val_4\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: val_5\n",
      "Key: 8, Value: val_8\n",
      "Key: 9, Value: val_9\n"
     ]
    }
   ],
   "source": [
    "for i in stringsDS.collect():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "Record = Row(\"key\", \"value\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "recordsDF = spark.createDataFrame(Record(i, \"val_\"+str(i)) for i in range(1,101))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "recordsDF.createOrReplaceTempView(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+\n",
      "|key| value|key| value|\n",
      "+---+------+---+------+\n",
      "|  2| val_2|  2| val_2|\n",
      "|  4| val_4|  4| val_4|\n",
      "|  5| val_5|  5| val_5|\n",
      "|  5| val_5|  5| val_5|\n",
      "|  5| val_5|  5| val_5|\n",
      "|  8| val_8|  8| val_8|\n",
      "|  9| val_9|  9| val_9|\n",
      "| 10|val_10| 10|val_10|\n",
      "| 11|val_11| 11|val_11|\n",
      "| 12|val_12| 12|val_12|\n",
      "| 12|val_12| 12|val_12|\n",
      "| 15|val_15| 15|val_15|\n",
      "| 15|val_15| 15|val_15|\n",
      "| 17|val_17| 17|val_17|\n",
      "| 18|val_18| 18|val_18|\n",
      "| 18|val_18| 18|val_18|\n",
      "| 19|val_19| 19|val_19|\n",
      "| 20|val_20| 20|val_20|\n",
      "| 24|val_24| 24|val_24|\n",
      "| 24|val_24| 24|val_24|\n",
      "+---+------+---+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM records r JOIN src s ON r.key = s.key\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE IF NOT EXISTS hive_records(key int, value string) STORED AS PARQUET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.table(\"src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"Overwrite\").saveAsTable(\"hive_records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"REFRESH TABLE hive_records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|key|  value|\n",
      "+---+-------+\n",
      "|238|val_238|\n",
      "| 86| val_86|\n",
      "|311|val_311|\n",
      "| 27| val_27|\n",
      "|165|val_165|\n",
      "|409|val_409|\n",
      "|255|val_255|\n",
      "|278|val_278|\n",
      "| 98| val_98|\n",
      "|484|val_484|\n",
      "|265|val_265|\n",
      "|193|val_193|\n",
      "|401|val_401|\n",
      "|150|val_150|\n",
      "|273|val_273|\n",
      "|224|val_224|\n",
      "|369|val_369|\n",
      "| 66| val_66|\n",
      "|128|val_128|\n",
      "|213|val_213|\n",
      "+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM hive_records\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir = \"/tmp/parquet_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(10).write.parquet(dataDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE EXTERNAL TABLE IF NOT EXISTS hive_bigints(id bigint) STORED AS PARQUET LOCATION '/tmp/parquet_data'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM hive_bigints\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"hive.exec.dynamic.partition\", \"true\")\n",
    "spark.conf.set(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"Overwrite\").partitionBy(\"key\").format(\"hive\").saveAsTable(\"hive_part_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.sql(\"SELECT * FROM hive_part_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|  value|key|\n",
      "+-------+---+\n",
      "|val_170|170|\n",
      "|val_368|368|\n",
      "|val_307|307|\n",
      "|val_307|307|\n",
      "|val_224|224|\n",
      "|val_224|224|\n",
      "| val_90| 90|\n",
      "| val_90| 90|\n",
      "| val_90| 90|\n",
      "|val_446|446|\n",
      "|val_333|333|\n",
      "|val_333|333|\n",
      "|val_365|365|\n",
      "|val_150|150|\n",
      "|val_164|164|\n",
      "|val_164|164|\n",
      "|val_128|128|\n",
      "|val_128|128|\n",
      "|val_128|128|\n",
      "|val_209|209|\n",
      "+-------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
