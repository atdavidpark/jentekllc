{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://bigdata:4040\n",
       "SparkContext available as 'sc' (version = 3.0.0-preview, master = local[*], app id = local-1577080719841)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark._\r\n",
       "import org.apache.spark.SparkContext._\r\n",
       "import org.apache.spark.rdd._\r\n",
       "import org.apache.spark.util.LongAccumulator\r\n",
       "import org.apache.log4j._\r\n",
       "import scala.collection.mutable.ArrayBuffer\r\n",
       "import org.apache.spark.sql._\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//George Jen, Jen Tek LLC\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.rdd._\n",
    "import org.apache.spark.util.LongAccumulator\n",
    "import org.apache.log4j._\n",
    "import scala.collection.mutable.ArrayBuffer\n",
    "import org.apache.spark.sql._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@44236390\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)\n",
    "val spark = SparkSession\n",
    "    .builder\n",
    "    .appName(\"csv2parquet\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.sql.warehouse.dir\", \"file:///d:/tmp\")\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------------+--------+---------------------------+--------------+----+\n",
      "|Ticker|Name                         |Exchange|CategoryName               |CategoryNumber|_c5 |\n",
      "+------+-----------------------------+--------+---------------------------+--------------+----+\n",
      "|AUB.AX|Austbrokers Holdings Limited |ASX     |Accident & Health Insurance|431           |null|\n",
      "|GLRE  |Greenlight Capital Re, Ltd.  |NMS     |Accident & Health Insurance|431           |null|\n",
      "|SFG   |StanCorp Financial Group Inc.|NYQ     |Accident & Health Insurance|431           |null|\n",
      "+------+-----------------------------+--------+---------------------------+--------------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ds: org.apache.spark.sql.DataFrame = [Ticker: string, Name: string ... 4 more fields]\r\n",
       "df: org.apache.spark.sql.DataFrame = [Ticker: string, Name: string ... 4 more fields]\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ds = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"quote\", \"\\\"\").load(\"D:/teaching/scala/ticker_symbol.csv\")\n",
    "val df: DataFrame = ds.toDF()\n",
    "df.show(3, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------------+--------+---------------------------+--------------+\n",
      "|Ticker|Name                         |Exchange|CategoryName               |CategoryNumber|\n",
      "+------+-----------------------------+--------+---------------------------+--------------+\n",
      "|AUB.AX|Austbrokers Holdings Limited |ASX     |Accident & Health Insurance|431           |\n",
      "|GLRE  |Greenlight Capital Re, Ltd.  |NMS     |Accident & Health Insurance|431           |\n",
      "|SFG   |StanCorp Financial Group Inc.|NYQ     |Accident & Health Insurance|431           |\n",
      "+------+-----------------------------+--------+---------------------------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df_with_datatype: org.apache.spark.sql.DataFrame = [Ticker: string, Name: string ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//When the CSV file was read into DataFrame, all fields are String, below is to cast it to\n",
    "//what the data should be, such as cast CategoryNumber to Int\n",
    "\n",
    "val df_with_datatype=df.selectExpr(\"Ticker\",\n",
    "                  \"Name\", \n",
    "                  \"Exchange\",\n",
    "                  \"CategoryName\",\n",
    "                  \"cast(CategoryNumber as int) CategoryNumber\")\n",
    "\n",
    "df_with_datatype.show(3, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Save the DataFrame to Parquet format, overwrite if existing.\n",
    "//Parquet is Columnar, good for Analytics query.\n",
    "\n",
    "df_with_datatype.write.mode(SaveMode.Overwrite).parquet(\"D:/teaching/scala/ticker_symbol.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------------+--------+---------------------------+--------------+\n",
      "|Ticker|Name                         |Exchange|CategoryName               |CategoryNumber|\n",
      "+------+-----------------------------+--------+---------------------------+--------------+\n",
      "|AUB.AX|Austbrokers Holdings Limited |ASX     |Accident & Health Insurance|431           |\n",
      "|GLRE  |Greenlight Capital Re, Ltd.  |NMS     |Accident & Health Insurance|431           |\n",
      "|SFG   |StanCorp Financial Group Inc.|NYQ     |Accident & Health Insurance|431           |\n",
      "+------+-----------------------------+--------+---------------------------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "read_parquet_df: org.apache.spark.sql.DataFrame = [Ticker: string, Name: string ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Read the Parquet data back and run SQL query on it\n",
    "\n",
    "val read_parquet_df = spark.read.parquet(\"D:/teaching/scala/ticker_symbol.parquet\")\n",
    "\n",
    "read_parquet_df.show(3, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ticker: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Exchange: string (nullable = true)\n",
      " |-- CategoryName: string (nullable = true)\n",
      " |-- CategoryNumber: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\r\n",
       "TickerSymbol: org.apache.spark.sql.DataFrame = [Ticker: string, Name: string ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "val TickerSymbol = read_parquet_df.toDF()\n",
    "TickerSymbol.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------------------------------+--------+-------------------------------+--------------+\n",
      "|Ticker|Name                                       |Exchange|CategoryName                   |CategoryNumber|\n",
      "+------+-------------------------------------------+--------+-------------------------------+--------------+\n",
      "|MSFT  |Microsoft Corporation                      |NMS     |Business Software & Services   |826           |\n",
      "|HPQ   |Hewlett-Packard Company                    |NYQ     |Diversified Computer Systems   |810           |\n",
      "|GE    |General Electric Company                   |NYQ     |Diversified Machinery          |622           |\n",
      "|IBM   |International Business Machines Corporation|NYQ     |Information Technology Services|824           |\n",
      "+------+-------------------------------------------+--------+-------------------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TickerSymbol.createOrReplaceTempView(\"TickerSymbol\")\n",
    "spark.sql(\"SELECT * from TickerSymbol where Ticker in ('IBM','MSFT','HPQ','GE')\").show(20,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
