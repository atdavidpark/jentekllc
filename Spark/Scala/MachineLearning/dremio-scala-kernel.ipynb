{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//start with pulling Vegas-viz library jars from Maven repository, your machines needs to connect to the \n",
    "// internet\n",
    "import $ivy.`org.vegas-viz::vegas:0.3.11`\n",
    "import $ivy.`org.vegas-viz::vegas-spark:0.3.11`\n",
    "//You will see lots of downloads from running the above 2 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Once done, you can import Vegas library for plotting\n",
    "import vegas._\n",
    "import vegas.render.WindowRenderer._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Almond/Jupyter-scala does not integrated with Spark, so you will need to integrate it manually\n",
    "//by download necessary jars from Maven, for the Spark libraries need to accomplish this demo\n",
    "import $ivy.`org.jupyter-scala::spark:0.4.2`\n",
    "import $ivy.`org.apache.spark::spark-sql:2.4.5`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Again lots of downloads from above 2 lines, once download is done, you are ready to import Spark libs\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.rdd._\n",
    "import org.apache.spark.util.LongAccumulator\n",
    "import org.apache.log4j._\n",
    "import org.apache.spark.sql._\n",
    "\n",
    "import org.apache.spark.sql.{Row, SparkSession}\n",
    "import org.apache.spark.sql.types.{DoubleType, StringType, StructField, StructType}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Create Spark session\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)\n",
    "val spark = SparkSession\n",
    "    .builder\n",
    "    .appName(\"Dremio-Vegas\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.sql.warehouse.dir\", \"file:///tmp\")\n",
    ".getOrCreate()\n",
    "\n",
    "import spark.implicits._\n",
    "import spark.sql\n",
    "\n",
    "//and SparkContext sc\n",
    "val sc = spark.sparkContext\n",
    "val sqlContext = new SQLContext(sc) \n",
    "import sqlContext.implicits._\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "//import java.nio.charset.Charset, set JDBC class path to point to jdbc jar file\n",
    "import java.nio.file.{FileSystem, FileSystems, Files}\n",
    "//import ammonite._\n",
    "import ammonite.ops._\n",
    "//val path = java.nio.file.FileSystems.getDefault().getPath(\"/opt/spark/jars/dremio-jdbc-driver-4.1.8-202003120636020140-9c2a6b13.jar\")\n",
    "val path = java.nio.file.FileSystems.getDefault().getPath(\"/opt/hadoop/dremio/dremio-jdbc-driver-3.0.6-201812082352540436-1f684f9.jar\")\n",
    "\n",
    "val x = ammonite.ops.Path(path)\n",
    "println(x)\n",
    "x.getClass\n",
    "interp.load.cp(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var jdbcDF = spark.read\n",
    "  .format(\"jdbc\")\n",
    "  .option(\"url\", \"jdbc:dremio:direct=10.0.2.15:31010\")\n",
    "//  .option(\"dbtable\", \"hive.src\")\n",
    "  .option(\"dbtable\", \"fraud.dremio.\\\"creditcard.csv\\\"\")\n",
    "  .option(\"header\",\"true\")\n",
    "  .option(\"user\", \"george\")\n",
    "  .option(\"password\", \"<redacted>\")\n",
    "  .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//df.write.mode(SaveMode.Overwrite).option(\"header\", \"true\").csv(\"/tmp/creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//val ds = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/tmp/creditcard.csv/*.csv\")\n",
    "//val jdbcDF: DataFrame = ds.toDF()\n",
    "//jdbcDF.show(3, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    "val rdd1=jdbcDF.rdd\n",
    "import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType};\n",
    "val schema = new StructType()\n",
    "  .add(StructField(\"key\", IntegerType, true))\n",
    "  .add(StructField(\"value\", StringType, true))\n",
    "val dfWithSchema = spark.createDataFrame(rdd1,schema).toDF(\"key\", \"value\")\n",
    "try{\n",
    "dfWithSchema.show()\n",
    "}\n",
    "*/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    "var jdbcDF1 = spark.read\n",
    "  .format(\"jdbc\")\n",
    "  .option(\"url\", \"jdbc:dremio:direct=10.0.2.15:31010\")\n",
    "//  .option(\"dbtable\", \"hive.src\")\n",
    "  .option(\"dbtable\", \"hive.realestate1\")\n",
    "  .option(\"user\", \"george\")\n",
    "  .option(\"password\", \"Jia19620112\")\n",
    "  .load()\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//jdbcDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df_with_datatype=jdbcDF.selectExpr(\n",
    "                  \"cast(V1 as double) V1\",\n",
    "                  \"cast(V2 as double) V2\",\n",
    "                  \"cast(V3 as double) V3\",\n",
    "                  \"cast(V4 as double) V4\",\n",
    "                  \"cast(V5 as double) V5\",\n",
    "                  \"cast(V6 as double) V6\",\n",
    "                  \"cast(V7 as double) V7\",\n",
    "                  \"cast(V8 as double) V8\",\n",
    "                  \"cast(V9 as double) V9\",\n",
    "                  \"cast(V10 as double) V10\",\n",
    "                  \"cast(V11 as double) V11\",\n",
    "                  \"cast(V12 as double) V12\",\n",
    "                  \"cast(V13 as double) V13\",\n",
    "                  \"cast(V14 as double) V14\",\n",
    "                  \"cast(V15 as double) V15\",\n",
    "                  \"cast(V16 as double) V16\",\n",
    "                  \"cast(V17 as double) V17\",\n",
    "                  \"cast(V18 as double) V18\",\n",
    "                  \"cast(V19 as double) V19\",\n",
    "                  \"cast(V20 as double) V20\",\n",
    "                  \"cast(V21 as double) V21\",\n",
    "                  \"cast(V22 as double) V22\",\n",
    "                  \"cast(V23 as double) V23\",\n",
    "                  \"cast(V24 as double) V24\",\n",
    "                  \"cast(V25 as double) V25\",\n",
    "                  \"cast(V26 as double) V26\",\n",
    "                  \"cast(V27 as double) V27\",\n",
    "                  \"cast(V28 as double) V28\",\n",
    "                  \"cast(Amount as double) Amount\",\n",
    "                  \"cast(Class as int) Label\"\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//df_with_datatype.createOrReplaceTempView(\"creditcardDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//val selected_feature_df=spark.sql(\"select V1,V2,V3,V4,V5,V6,V7,V8,V9,V10,V11,V12,V13,V14,V15,V16,V17,V18,V19,V20,V21,V22,V23,V24,V25,V26,V27,V28,Amount from creditcardDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//val label_df=spark.sql(\"select Class from creditcardDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`org.apache.spark::spark-mllib:2.4.5`\n",
    "import org.apache.spark.ml.feature.VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val vectorAssembler = new VectorAssembler().setInputCols(Array(\n",
    "                  \"V1\",\n",
    "                  \"V2\",\n",
    "                  \"V3\",\n",
    "                  \"V4\",\n",
    "                  \"V5\",\n",
    "                  \"V6\",\n",
    "                  \"V7\",\n",
    "                  \"V8\",\n",
    "                  \"V9\",\n",
    "                  \"V10\",\n",
    "                  \"V11\",\n",
    "                  \"V12\",\n",
    "                  \"V13\",\n",
    "                  \"V14\",\n",
    "                  \"V15\",\n",
    "                  \"V16\",\n",
    "                  \"V17\",\n",
    "                  \"V18\",\n",
    "                  \"V19\",\n",
    "                  \"V20\",\n",
    "                  \"V21\",\n",
    "                  \"V22\",\n",
    "                  \"V23\",\n",
    "                  \"V24\",\n",
    "                  \"V25\",\n",
    "                  \"V26\",\n",
    "                  \"V27\",\n",
    "                  \"V28\",\n",
    "                  \"Amount\"\n",
    "     )).setOutputCol(\"features\")\n",
    "//  .setInputCols(Array(\"C\"))\n",
    "  \n",
    "\n",
    "//setInputCols(Array(\"DateCat\", \"Userid\")).setOutputCol(\"rawfeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var vector_df = vectorAssembler.setHandleInvalid(\"skip\").transform(df_with_datatype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_df.select(\"features\",\"Label\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_df = vector_df.select(\"features\", \"Label\").sort(\"features\").na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val splits = vector_df.randomSplit(Array(0.7,0.3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val train_df=splits(0)\n",
    "val test_df = splits(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.getClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.{Pipeline, PipelineStage}\n",
    "import org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel}\n",
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.collection.mutable\n",
    "val maxIter=100\n",
    "val lr = new LogisticRegression()\n",
    "      .setFeaturesCol(\"features\")\n",
    "      .setLabelCol(\"Label\")\n",
    "      .setMaxIter(maxIter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val model_lr = lr.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val prediction_lr = model_lr.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val my_eval_aoc = new BinaryClassificationEvaluator()\n",
    "  .setLabelCol(\"Label\")\n",
    "  .setRawPredictionCol(\"prediction\").setMetricName(\"areaUnderROC\")\n",
    "\n",
    "my_eval_aoc.evaluate(prediction_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "var my_mc_f1 = new MulticlassClassificationEvaluator().setPredictionCol(\"prediction\").setLabelCol(\"Label\").setMetricName(\"f1\")\n",
    "my_mc_f1.evaluate(prediction_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val my_mc_accu = new MulticlassClassificationEvaluator().setPredictionCol(\"prediction\").setLabelCol(\"Label\").setMetricName(\"accuracy\")\n",
    "my_mc_accu.evaluate(prediction_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val train_fit_lr = prediction_lr.select(\"Label\",\"prediction\")\n",
    "train_fit_lr.groupBy(\"Label\",\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
