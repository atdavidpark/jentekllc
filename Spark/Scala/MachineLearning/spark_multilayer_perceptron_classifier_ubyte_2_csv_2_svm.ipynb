{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://master.hadoop.lan:4041\n",
       "SparkContext available as 'sc' (version = 3.0.0-preview, master = local[*], app id = local-1587227863933)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import java.io._\n",
       "import spark.implicits._\n",
       "mnistFileConvertUByteToCSV: (imageFileName: String, labelFileName: String, recNum: Int, csvFileName: String)Unit\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.io._\n",
    "import spark.implicits._\n",
    "def mnistFileConvertUByteToCSV(imageFileName: String, labelFileName: String, recNum: Int, csvFileName: String): Unit={\n",
    "    val rows = recNum\n",
    "    val cols = 28*28\n",
    "    var data = None: Option[FileInputStream]\n",
    "    var label = None: Option[FileInputStream]\n",
    "    var out = None: Option[FileOutputStream]\n",
    "    val features = Array.ofDim[Int](rows, cols)\n",
    "    val file = new File(csvFileName)\n",
    "    val output = new BufferedWriter(new FileWriter(file))\n",
    "\n",
    "    val target=Array.ofDim[Int](rows,1)\n",
    "    try {\n",
    "        data = Some(new FileInputStream(imageFileName))\n",
    "        label = Some(new FileInputStream(labelFileName))\n",
    "\n",
    "    \n",
    "        var c = 0\n",
    "        for (_<-0 until 16)\n",
    "            data.get.read\n",
    "        for (_<-0 until 8)\n",
    "            label.get.read\n",
    "        c = 0\n",
    "        for (i<-0 until rows)\n",
    "          {\n",
    "            c=label.get.read\n",
    "            target(i)(0)=c\n",
    "            output.write(c.toString)\n",
    "            output.write(\",\")\n",
    "        \n",
    "           for (j<-0 until cols)\n",
    "            {\n",
    "            c = data.get.read\n",
    "            features(i)(j)=c\n",
    "            output.write(c.toString)\n",
    "            if (j<cols-1)\n",
    "                output.write(\",\")\n",
    "            else\n",
    "                output.write(\"\\n\")\n",
    "\n",
    "            }\n",
    "        if (i%1000==0)\n",
    "            println(s\"Line number: $i\")\n",
    "    } } catch {\n",
    "        case e: IOException => e.printStackTrace\n",
    "    } finally {\n",
    "        println(\"entered finally ...\")\n",
    "        if (data.isDefined) data.get.close\n",
    "        if (label.isDefined)\n",
    "        {\n",
    "           label.get.close\n",
    "          \n",
    "        }\n",
    "         output.close\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line number: 0\n",
      "Line number: 1000\n",
      "Line number: 2000\n",
      "Line number: 3000\n",
      "Line number: 4000\n",
      "Line number: 5000\n",
      "Line number: 6000\n",
      "Line number: 7000\n",
      "Line number: 8000\n",
      "Line number: 9000\n",
      "Line number: 10000\n",
      "Line number: 11000\n",
      "Line number: 12000\n",
      "Line number: 13000\n",
      "Line number: 14000\n",
      "Line number: 15000\n",
      "Line number: 16000\n",
      "Line number: 17000\n",
      "Line number: 18000\n",
      "Line number: 19000\n",
      "Line number: 20000\n",
      "Line number: 21000\n",
      "Line number: 22000\n",
      "Line number: 23000\n",
      "Line number: 24000\n",
      "Line number: 25000\n",
      "Line number: 26000\n",
      "Line number: 27000\n",
      "Line number: 28000\n",
      "Line number: 29000\n",
      "Line number: 30000\n",
      "Line number: 31000\n",
      "Line number: 32000\n",
      "Line number: 33000\n",
      "Line number: 34000\n",
      "Line number: 35000\n",
      "Line number: 36000\n",
      "Line number: 37000\n",
      "Line number: 38000\n",
      "Line number: 39000\n",
      "Line number: 40000\n",
      "Line number: 41000\n",
      "Line number: 42000\n",
      "Line number: 43000\n",
      "Line number: 44000\n",
      "Line number: 45000\n",
      "Line number: 46000\n",
      "Line number: 47000\n",
      "Line number: 48000\n",
      "Line number: 49000\n",
      "Line number: 50000\n",
      "Line number: 51000\n",
      "Line number: 52000\n",
      "Line number: 53000\n",
      "Line number: 54000\n",
      "Line number: 55000\n",
      "Line number: 56000\n",
      "Line number: 57000\n",
      "Line number: 58000\n",
      "Line number: 59000\n",
      "entered finally ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "x: String = /home/bigdata2/libsvm/train-images-idx3-ubyte\n",
       "y: String = /home/bigdata2/libsvm/train-labels-idx1-ubyte\n",
       "z: String = /home/bigdata2/libsvm/mnist_train.csv\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var x=\"/home/bigdata2/libsvm/train-images-idx3-ubyte\"\n",
    "var y=\"/home/bigdata2/libsvm/train-labels-idx1-ubyte\"\n",
    "var z=\"/home/bigdata2/libsvm/mnist_train.csv\"\n",
    "\n",
    "mnistFileConvertUByteToCSV(x,y,60000,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line number: 0\n",
      "Line number: 1000\n",
      "Line number: 2000\n",
      "Line number: 3000\n",
      "Line number: 4000\n",
      "Line number: 5000\n",
      "Line number: 6000\n",
      "Line number: 7000\n",
      "Line number: 8000\n",
      "Line number: 9000\n",
      "entered finally ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "x: String = /home/bigdata2/libsvm/t10k-images-idx3-ubyte\n",
       "y: String = /home/bigdata2/libsvm/t10k-labels-idx1-ubyte\n",
       "z: String = /home/bigdata2/libsvm/t10k.csv\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=\"/home/bigdata2/libsvm/t10k-images-idx3-ubyte\"\n",
    "y=\"/home/bigdata2/libsvm/t10k-labels-idx1-ubyte\"\n",
    "z=\"/home/bigdata2/libsvm/t10k.csv\"\n",
    "\n",
    "mnistFileConvertUByteToCSV(x,y,10000,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark._\n",
       "import org.apache.spark.SparkContext._\n",
       "import org.apache.spark.rdd._\n",
       "import org.apache.spark.util.LongAccumulator\n",
       "import org.apache.log4j._\n",
       "import scala.collection.mutable.ArrayBuffer\n",
       "import org.apache.spark.sql._\n",
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.rdd._\n",
    "import org.apache.spark.util.LongAccumulator\n",
    "import org.apache.log4j._\n",
    "import scala.collection.mutable.ArrayBuffer\n",
    "import org.apache.spark.sql._\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "makeLibsvm: (a: Array[String])String\n",
       "csvFile: org.apache.spark.rdd.RDD[String] = file:///home/bigdata2/libsvm/mnist_train.csv MapPartitionsRDD[1] at textFile at <console>:57\n",
       "libsvm: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[3] at map at <console>:58\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def makeLibsvm(a:Array[String]):String ={\n",
    "  var result=a(0)+\" \"\n",
    "  for(i<-1 to a.size.toInt-1) \n",
    "  result=result+i+\":\"+a(i)(0)+\" \"\n",
    "  return result\n",
    "}\n",
    "var csvFile=sc.textFile(\"file:///home/bigdata2/libsvm/mnist_train.csv\")\n",
    "var libsvm=csvFile.map(line => line.split(',')).map(i=>makeLibsvm(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "libsvm.saveAsTextFile(\"file:///home/bigdata2/libsvm/mnist_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "csvFile: org.apache.spark.rdd.RDD[String] = file:///home/bigdata2/libsvm/t10k.csv MapPartitionsRDD[6] at textFile at <console>:55\n",
       "libsvm: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[8] at map at <console>:56\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csvFile=sc.textFile(\"file:///home/bigdata2/libsvm/t10k.csv\")\n",
    "libsvm=csvFile.map(line => line.split(',')).map(i=>makeLibsvm(i))\n",
    "libsvm.saveAsTextFile(\"file:///home/bigdata2/libsvm/mnist_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.io._\n",
    "val libsvmFile = new File(\"/home/bigdata2/libsvm/mnist_train.libsvm\")\n",
    "val svmout = new BufferedWriter(new FileWriter(libsvmFile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//val libsvmArray=libsvm.collect\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    "for (i<-0 until libsvmArray.size)\n",
    "{\n",
    "    svmout.write(libsvmArray(i))\n",
    "    svmout.write(\"\\n\")\n",
    "    if (i%1000==0)\n",
    "       print(s\"line $i\")\n",
    "    \n",
    "}\n",
    "svmout.close\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    "val data = spark.read.format(\"libsvm\")\n",
    "  .load(\"file:///home/bigdata2/libsvm/mnist_train/mnist_train.libsvm\")\n",
    "\n",
    "// Split the data into train and test\n",
    "val splits = data.randomSplit(Array(0.6, 0.4), seed = 1234L)\n",
    "val train = splits(0)\n",
    "val test = splits(1)\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n",
       "test: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val train = spark.read.format(\"libsvm\")\n",
    "  .load(\"file:///home/bigdata2/libsvm/mnist_train.libsvm\")\n",
    "val test = spark.read.format(\"libsvm\")\n",
    "  .load(\"file:///home/bigdata2/libsvm/mnist_test.libsvm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  5.0|(784,[152,153,154...|\n",
      "|  0.0|(784,[127,128,129...|\n",
      "|  4.0|(784,[160,161,162...|\n",
      "+-----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "layers: Array[Int] = Array(784, 784, 784, 10)\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//val layers = Array[Int](4, 5, 4, 3)\n",
    "val layers = Array[Int](784, 784, 784, 10)\n",
    "\n",
    "// create the trainer and set its parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.MultilayerPerceptronClassifier\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "trainer: org.apache.spark.ml.classification.MultilayerPerceptronClassifier = mlpc_c2b830eadc3a\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.MultilayerPerceptronClassifier\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "val trainer = new MultilayerPerceptronClassifier()\n",
    "  .setLayers(layers)\n",
    "  .setBlockSize(128)\n",
    "  .setSeed(1234L)\n",
    "  .setMaxIter(100)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel = mlpc_c2b830eadc3a\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// train the model\n",
    "val model = trainer.fit(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.9193\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "result: org.apache.spark.sql.DataFrame = [label: double, features: vector ... 3 more fields]\n",
       "predictionAndLabels: org.apache.spark.sql.DataFrame = [prediction: double, label: double]\n",
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_815e90ab92c4\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// compute accuracy on the test set\n",
    "val result = model.transform(test)\n",
    "val predictionAndLabels = result.select(\"prediction\", \"label\")\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setMetricName(\"accuracy\")\n",
    "\n",
    "println(s\"Test set accuracy = ${evaluator.evaluate(predictionAndLabels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.toDF.createOrReplaceTempView(\"deep_learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  2.0|(784,[94,95,96,97...|[-7.3486569212363...|[1.88751449816836...|       2.0|\n",
      "|  2.0|(784,[124,125,126...|[-4.9435463442057...|[1.24313405935197...|       2.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from deep_learning where prediction = label and label = 2.0 limit 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|accuracy|\n",
      "+--------+\n",
      "|  0.9193|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"with x as (select count(*) as count_x from deep_learning where prediction = label),y as (select count(*) as total from deep_learning) select count_x/total as accuracy from x,y\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|accuracy|\n",
      "+--------+\n",
      "|  0.9193|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select (select count(*) from deep_learning where label=prediction)/count(*) as accuracy from deep_learning\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
