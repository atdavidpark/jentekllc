{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://master.hadoop.lan:4042\n",
       "SparkContext available as 'sc' (version = 3.0.0-preview, master = local[*], app id = local-1580541939438)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark._\n",
       "import org.apache.spark.graphx._\n",
       "import org.apache.spark.rdd.RDD\n",
       "import org.apache.log4j._\n",
       "import org.apache.spark.sql._\n",
       "import org.apache.spark.graphx.{Graph, VertexRDD}\n",
       "import org.apache.spark.graphx.util.GraphGenerators\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.graphx._\n",
    "// To make some of the examples work we will also need RDD\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.log4j._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.graphx.{Graph, VertexRDD}\n",
    "import org.apache.spark.graphx.util.GraphGenerators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@7bceb4e4\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)\n",
    "val spark = SparkSession\n",
    "    .builder\n",
    "    .appName(\"graphx\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.sql.warehouse.dir\", \"file:///tmp\")\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sc: org.apache.spark.SparkContext = org.apache.spark.SparkContext@58b22b09\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "users: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, (String, String))] = ParallelCollectionRDD[0] at parallelize at <console>:42\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val users: RDD[(VertexId, (String, String))] =\n",
    "  sc.parallelize(Array((1L, (\"jack\", \"owner\")), (2L, (\"george\", \"clerk\")),\n",
    "                       (3L, (\"mary\", \"sales\")), (4L, (\"sherry\", \"owner wife\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,(mary,sales))\n",
      "(2,(george,clerk))\n",
      "(4,(sherry,owner wife))\n",
      "(1,(jack,owner))\n"
     ]
    }
   ],
   "source": [
    "users.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relationships: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[String]] = ParallelCollectionRDD[1] at parallelize at <console>:42\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val relationships: RDD[Edge[String]] =\n",
    "  sc.parallelize(Array(Edge(1L, 2L, \"boss\"),    Edge(1L, 3L, \"boss\"),\n",
    "                       Edge(2L, 3L, \"coworker\"), Edge(4L, 1L, \"boss\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge(1,3,boss)\n",
      "Edge(4,1,boss)\n",
      "Edge(1,2,boss)\n",
      "Edge(2,3,coworker)\n"
     ]
    }
   ],
   "source": [
    "relationships.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultUser: (String, String) = (\"\",Missing)\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val defaultUser = (\"\", \"Missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graph: org.apache.spark.graphx.Graph[(String, String),String] = org.apache.spark.graphx.impl.GraphImpl@4fd2552e\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val graph = Graph(users, relationships, defaultUser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jack, owner is the boss of george, clerk\n",
      "jack, owner is the boss of mary, sales\n",
      "george, clerk is the coworker of mary, sales\n",
      "sherry, owner wife is the boss of jack, owner\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "facts: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[19] at map at <console>:48\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*\n",
    "The EdgeTriplet class extends the Edge class by adding the srcAttr and dstAttr members \n",
    "which contain the source and destination properties respectively. \n",
    "We can use the triplet view of a graph to render a collection of strings describing relationships between users.\n",
    "*/\n",
    "\n",
    "val facts: RDD[String] =\n",
    "  graph.triplets.map(triplet =>\n",
    "    triplet.srcAttr._1 + \", \"+ triplet.srcAttr._2 + \" is the \" + triplet.attr + \" of \" + triplet.dstAttr._1+\", \"+triplet.dstAttr._2)\n",
    "facts.collect.foreach(println(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
